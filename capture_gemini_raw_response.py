#!/usr/bin/env python3
"""
Gemini åŸå§‹å“åº”æ•è·å·¥å…·

åŠŸèƒ½ï¼š
1. æ•è· Gemini API è¿”å›çš„åŸå§‹å­—èŠ‚æµ
2. è§£æå¹¶ä¿å­˜å®Œæ•´çš„å“åº”æ•°æ®
3. å¯¹æ¯”å¤„ç†å‰åçš„æ•°æ®å·®å¼‚

ä½¿ç”¨æ–¹æ³•ï¼š
1. åœ¨ stream/interceptors.py çš„ parse_response æ–¹æ³•å¼€å§‹å¤„æ·»åŠ æ—¥å¿—è®°å½•
2. è¿è¡ŒæœåŠ¡å¹¶è§¦å‘ä¸€æ¬¡è¯·æ±‚
3. è¿è¡Œæ­¤è„šæœ¬åˆ†ææ—¥å¿—ï¼špython3 capture_gemini_raw_response.py
"""

import json
import re
from pathlib import Path
from datetime import datetime


def extract_raw_responses_from_log(log_file=None):
    """ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–åŸå§‹å“åº”æ•°æ®"""

    print("=" * 80)
    print("ğŸ” Gemini åŸå§‹å“åº”æ•è·åˆ†æ")
    print("=" * 80)
    print()

    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ—¥å¿—æ–‡ä»¶
    if log_file is None:
        log_files = [
            'logs/proxy_server.log',
            'logs/headless.log',
            'logs/app.log',
        ]
        for f in log_files:
            if Path(f).exists():
                log_file = f
                print(f"âœ… ä½¿ç”¨æ—¥å¿—æ–‡ä»¶: {log_file}")
                print()
                break

    if not Path(log_file).exists():
        print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
        return

    with open(log_file, 'r', encoding='utf-8') as f:
        log_content = f.read()

    # æŸ¥æ‰¾åŸå§‹å“åº”æ•°æ®çš„æ—¥å¿—
    # æ ¼å¼: [RAW_RESPONSE] chunk_X: b'...'
    # ä¿®å¤ï¼šåŒ¹é…åˆ°è¡Œå°¾ï¼Œå¤„ç†æ•°æ®ä¸­çš„è½¬ä¹‰å¼•å·
    raw_pattern = r'\[RAW_RESPONSE\] chunk_(\d+): (b\'.+?)$'

    chunks = []
    for match in re.finditer(raw_pattern, log_content, re.MULTILINE):
        chunk_num = int(match.group(1))
        chunk_data = match.group(2)
        chunks.append((chunk_num, chunk_data))

    if not chunks:
        print("âš ï¸  æœªæ‰¾åˆ° [RAW_RESPONSE] æ ‡è®°çš„æ—¥å¿—")
        print()
        print("è¯·åœ¨ stream/interceptors.py çš„ parse_response() æ–¹æ³•å¼€å§‹å¤„æ·»åŠ ï¼š")
        print()
        print("  self.logger.info(f'[RAW_RESPONSE] chunk_{self._parse_call_count}: {response_data}')")
        print()
        return

    print(f"âœ… æ‰¾åˆ° {len(chunks)} ä¸ªåŸå§‹å“åº” chunk")
    print()

    # ä¿å­˜åˆ°æ–‡ä»¶
    output_dir = Path('debug_output')
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = output_dir / f'gemini_raw_response_{timestamp}.txt'

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("Gemini API åŸå§‹å“åº”æ•°æ®\n")
        f.write(f"æ•è·æ—¶é—´: {datetime.now()}\n")
        f.write("=" * 80 + "\n\n")

        for chunk_num, chunk_data in chunks:
            f.write(f"--- Chunk {chunk_num} ---\n")
            f.write(f"åŸå§‹æ•°æ®: {chunk_data}\n")

            # å°è¯•è§£æ
            try:
                # ç§»é™¤ b' å‰ç¼€å’Œ ' åç¼€
                if chunk_data.startswith("b'") and chunk_data.endswith("'"):
                    chunk_bytes = eval(chunk_data)  # å®‰å…¨ï¼šè¿™æ˜¯ä»æ—¥å¿—ä¸­è¯»å–çš„

                    f.write(f"å­—èŠ‚é•¿åº¦: {len(chunk_bytes)}\n")

                    # å°è¯•è§£æ JSON
                    # ä¿®å¤ï¼šåŒ¹é…å®Œæ•´çš„ JSON å—ç»“æ„
                    # æ ¼å¼ï¼š[[[[[[null,"content"]],"model"]]],...]
                    pattern = rb'\[\[\[null,"[^"]*(?:\\.[^"]*)*"(?:\]|,).*?\],"model"\]\]'
                    matches = re.findall(pattern, chunk_bytes, re.DOTALL)

                    if matches:
                        f.write(f"JSON å—æ•°é‡: {len(matches)}\n")
                        for i, match in enumerate(matches):
                            f.write(f"\n  JSON å— {i+1}:\n")
                            f.write(f"  åŸå§‹: {match}\n")

                            try:
                                json_data = json.loads(match, strict=False)
                                payload = json_data[0][0]
                                if payload and len(payload) > 1:
                                    content = payload[1]
                                    f.write(f"  æå–å†…å®¹: {repr(content)}\n")
                                    f.write(f"  å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦\n")
                            except Exception as e:
                                f.write(f"  âŒ è§£æå¤±è´¥: {e}\n")
                    else:
                        f.write("âš ï¸  æœªåŒ¹é…åˆ° JSON å—\n")
                        f.write(f"åŸå§‹å­—èŠ‚å‰ 200 å­—ç¬¦: {chunk_bytes[:200]}\n")

            except Exception as e:
                f.write(f"âŒ chunk è§£æé”™è¯¯: {e}\n")

            f.write("\n")

    print(f"âœ… è¯¦ç»†åˆ†æå·²ä¿å­˜åˆ°: {output_file}")
    print()

    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ‘˜è¦
    print("ğŸ“Š æ•°æ®æ‘˜è¦:")
    print("-" * 80)

    total_bytes = 0
    total_content_chars = 0

    for chunk_num, chunk_data in chunks:
        try:
            if chunk_data.startswith("b'") and chunk_data.endswith("'"):
                chunk_bytes = eval(chunk_data)
                total_bytes += len(chunk_bytes)

                # æå–å†…å®¹
                pattern = rb'\[\[\[null,.*?],"model"]]'
                matches = re.findall(pattern, chunk_bytes)

                for match in matches:
                    try:
                        json_data = json.loads(match, strict=False)
                        payload = json_data[0][0]
                        if payload and len(payload) > 1:
                            content = payload[1]
                            total_content_chars += len(content)
                            print(f"  Chunk {chunk_num}: {len(content)} å­—ç¬¦ | {repr(content[:50])}...")
                    except:
                        pass
        except:
            pass

    print("-" * 80)
    print(f"ğŸ“ˆ æ€»è®¡: {len(chunks)} ä¸ª chunk, {total_bytes} å­—èŠ‚, {total_content_chars} å­—ç¬¦å†…å®¹")
    print()


def analyze_interceptor_processing():
    """åˆ†ææ‹¦æˆªå™¨å¤„ç†å‰åçš„æ•°æ®å¯¹æ¯”"""

    print("=" * 80)
    print("ğŸ”¬ æ‹¦æˆªå™¨å¤„ç†åˆ†æ")
    print("=" * 80)
    print()

    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ—¥å¿—æ–‡ä»¶
    log_files = [
        'logs/proxy_server.log',
        'logs/app.log',
        'logs/headless.log',
    ]
    log_file = None
    for f in log_files:
        if Path(f).exists():
            log_file = f
            break

    if not log_file or not Path(log_file).exists():
        print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
        return

    with open(log_file, 'r', encoding='utf-8') as f:
        log_content = f.read()

    # æŸ¥æ‰¾ç»Ÿè®¡ä¿¡æ¯
    stats_pattern = r'\[ç»Ÿè®¡\] è°ƒç”¨: (\d+) æ¬¡, æå–: (\d+) å­—èŠ‚, å‘é€: (\d+) å­—èŠ‚, ç¼“å†²åŒº: (\d+) å­—èŠ‚'
    final_pattern = r'\[æœ€ç»ˆç»Ÿè®¡\] æ€»è°ƒç”¨: (\d+), æ€»æå–: (\d+) å­—èŠ‚, æ€»å‘é€: (\d+) å­—èŠ‚, ä¸¢å¤±: (-?\d+) å­—èŠ‚ \(([\d.]+)%\)'

    stats_matches = list(re.finditer(stats_pattern, log_content))
    final_matches = list(re.finditer(final_pattern, log_content))

    if not stats_matches and not final_matches:
        print("âš ï¸  æœªæ‰¾åˆ°ç»Ÿè®¡ä¿¡æ¯")
        print("è¯·ç¡®è®¤ interceptors.py ä¸­å·²å¯ç”¨ç»Ÿè®¡æ¨¡å¼")
        return

    if stats_matches:
        print(f"âœ… æ‰¾åˆ° {len(stats_matches)} æ¡ä¸­é—´ç»Ÿè®¡")
        print()
        print("ğŸ“Š å¤„ç†è¿›åº¦:")
        for match in stats_matches[-5:]:  # æ˜¾ç¤ºæœ€å 5 æ¡
            calls, extracted, sent, buffered = match.groups()
            print(f"  è°ƒç”¨ {calls} æ¬¡: æå– {extracted}B, å‘é€ {sent}B, ç¼“å†² {buffered}B")
        print()

    if final_matches:
        print("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        for match in final_matches[-3:]:  # æ˜¾ç¤ºæœ€å 3 æ¬¡è¯·æ±‚
            calls, extracted, sent, lost, percent = match.groups()
            print(f"  æ€»è°ƒç”¨: {calls}, æå–: {extracted}B, å‘é€: {sent}B, ä¸¢å¤±: {lost}B ({percent}%)")
        print()

        # åˆ†ææœ€åä¸€æ¬¡
        last_match = final_matches[-1]
        calls, extracted, sent, lost, percent = last_match.groups()

        extracted_int = int(extracted)
        sent_int = int(sent)
        lost_int = int(lost)
        percent_float = float(percent)

        print("ğŸ” æœ€åä¸€æ¬¡è¯·æ±‚åˆ†æ:")
        if extracted_int == 0:
            print("  âŒ ä» Gemini API æå–çš„æ•°æ®ä¸º 0 å­—èŠ‚")
            print("  â†’ å¯èƒ½åŸå› : æ­£åˆ™è¡¨è¾¾å¼ä¸åŒ¹é… / JSON è§£æå…¨éƒ¨å¤±è´¥")
        elif sent_int == 0:
            print("  âŒ å‘é€ç»™å®¢æˆ·ç«¯çš„æ•°æ®ä¸º 0 å­—èŠ‚")
            print("  â†’ å¯èƒ½åŸå› : ç¼“å†²é€»è¾‘é˜»å¡äº†æ‰€æœ‰æ•°æ®")
        elif percent_float > 50:
            print(f"  âš ï¸  æ•°æ®ä¸¢å¤±ç‡é«˜è¾¾ {percent}%")
            print("  â†’ å¯èƒ½åŸå› : ç¼“å†²çª—å£é€»è¾‘è¿‡åº¦ç¼“å†²")
        elif percent_float < 10:
            print(f"  âœ… æ•°æ®ä¸¢å¤±ç‡è¾ƒä½ ({percent}%)")
            print("  â†’ å¯èƒ½æ˜¯æ­£å¸¸çš„å·¥å…·è°ƒç”¨ JSON å—è¢«éšè—")
        else:
            print(f"  âš ï¸  æ•°æ®ä¸¢å¤±ç‡: {percent}%")
            print("  â†’ éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")


if __name__ == '__main__':
    print()
    print("ğŸš€ å¯åŠ¨ Gemini åŸå§‹å“åº”åˆ†æå·¥å…·")
    print()

    # æ­¥éª¤ 1: æå–åŸå§‹å“åº”
    extract_raw_responses_from_log()

    # æ­¥éª¤ 2: åˆ†æå¤„ç†æµç¨‹
    analyze_interceptor_processing()

    print()
    print("=" * 80)
    print("âœ… åˆ†æå®Œæˆ")
    print("=" * 80)
    print()
    print("ğŸ“ ä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹ debug_output/ ç›®å½•ä¸‹çš„è¯¦ç»†åˆ†ææ–‡ä»¶")
    print("  2. å¯¹æ¯”åŸå§‹æ•°æ®å’Œæœ€ç»ˆç»Ÿè®¡ï¼Œæ‰¾å‡ºæ•°æ®ä¸¢å¤±çš„ç¯èŠ‚")
    print("  3. æ ¹æ®åˆ†æç»“æœè°ƒæ•´ interceptors.py ä¸­çš„å¤„ç†é€»è¾‘")
    print()
