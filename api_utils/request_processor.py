"""
è¯·æ±‚å¤„ç†å™¨æ¨¡å—
åŒ…å«æ ¸å¿ƒçš„è¯·æ±‚å¤„ç†é€»è¾‘
"""

import asyncio
import json
import os
import random
import time
from typing import Optional, Tuple, Callable, AsyncGenerator, List, Any
from asyncio import Event, Future

from fastapi import HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from playwright.async_api import Page as AsyncPage, Locator, Error as PlaywrightAsyncError, expect as expect_async

# --- é…ç½®æ¨¡å—å¯¼å…¥ ---
from config import (
    MODEL_NAME,
    SUBMIT_BUTTON_SELECTOR,
)
from config import ONLY_COLLECT_CURRENT_USER_ATTACHMENTS, UPLOAD_FILES_DIR

# --- modelsæ¨¡å—å¯¼å…¥ ---
from models import ChatCompletionRequest, ClientDisconnectedError

# --- browser_utilsæ¨¡å—å¯¼å…¥ ---
from browser_utils import (
    switch_ai_studio_model,
    save_error_snapshot
)

# --- api_utilsæ¨¡å—å¯¼å…¥ ---
from .utils import (
    validate_chat_request,
    prepare_combined_prompt,
    use_stream_response,
    calculate_usage_stats,
    maybe_execute_tools,
    extract_tool_calls_from_text,
    format_tool_calls_for_response,
)
from browser_utils.page_controller import PageController
from .context_types import RequestContext
from .response_generators import gen_sse_from_aux_stream, gen_sse_from_playwright
from .response_payloads import build_chat_completion_response_json
from .model_switching import analyze_model_requirements as ms_analyze, handle_model_switching as ms_switch, handle_parameter_cache as ms_param_cache
from .page_response import locate_response_elements

from .common_utils import random_id as _random_id
from .client_connection import (
    test_client_connection as _test_client_connection,
    setup_disconnect_monitoring as _setup_disconnect_monitoring,
)
from .context_init import initialize_request_context as _init_request_context

_initialize_request_context = _init_request_context

# Error helpers
from .error_utils import (
    bad_request,
    client_disconnected,
    upstream_error,
    server_error,
)


async def _analyze_model_requirements(req_id: str, context: RequestContext, request: ChatCompletionRequest) -> RequestContext:
    """ä»£ç†åˆ° model_switching.analyze_model_requirements"""
    return await ms_analyze(req_id, context, request.model, MODEL_NAME)


# ç›´æ¥ä½¿ç”¨å¯¼å…¥çš„å®ç°

# ç›´æ¥ä½¿ç”¨å¯¼å…¥çš„å®ç°


async def _validate_page_status(req_id: str, context: RequestContext, check_client_disconnected: Callable) -> None:
    """éªŒè¯é¡µé¢çŠ¶æ€"""
    page = context['page']
    is_page_ready = context['is_page_ready']
    
    if not page or page.is_closed() or not is_page_ready:
        raise HTTPException(status_code=503, detail=f"[{req_id}] AI Studio é¡µé¢ä¸¢å¤±æˆ–æœªå°±ç»ªã€‚", headers={"Retry-After": "30"})
    
    check_client_disconnected("Initial Page Check")


async def _handle_model_switching(req_id: str, context: RequestContext, check_client_disconnected: Callable) -> RequestContext:
    """ä»£ç†åˆ° model_switching.handle_model_switching"""
    return await ms_switch(req_id, context)


async def _handle_model_switch_failure(req_id: str, page: AsyncPage, model_id_to_use: str, model_before_switch: str, logger) -> None:
    """å¤„ç†æ¨¡å‹åˆ‡æ¢å¤±è´¥çš„æƒ…å†µ"""
    import server
    
    logger.warning(f"[{req_id}] âŒ æ¨¡å‹åˆ‡æ¢è‡³ {model_id_to_use} å¤±è´¥ã€‚")
    # å°è¯•æ¢å¤å…¨å±€çŠ¶æ€
    server.current_ai_studio_model_id = model_before_switch
    
    raise HTTPException(
        status_code=422,
        detail=f"[{req_id}] æœªèƒ½åˆ‡æ¢åˆ°æ¨¡å‹ '{model_id_to_use}'ã€‚è¯·ç¡®ä¿æ¨¡å‹å¯ç”¨ã€‚"
    )


async def _handle_parameter_cache(req_id: str, context: RequestContext) -> None:
    """ä»£ç†åˆ° model_switching.handle_parameter_cache"""
    await ms_param_cache(req_id, context)


async def _prepare_and_validate_request(
    req_id: str,
    request: ChatCompletionRequest,
    check_client_disconnected: Callable,
) -> Tuple[str, List[Optional[str]]]:
    """å‡†å¤‡å’ŒéªŒè¯è¯·æ±‚ï¼Œè¿”å› (ç»„åˆæç¤º, å›¾ç‰‡è·¯å¾„åˆ—è¡¨)ã€‚"""
    try:
        validate_chat_request(request.messages, req_id)
    except ValueError as e:
        raise bad_request(req_id, f"æ— æ•ˆè¯·æ±‚: {e}")
    
    prepared_prompt, images_list = prepare_combined_prompt(request.messages, req_id, getattr(request, 'tools', None), getattr(request, 'tool_choice', None))
    # åŸºäº tools/tool_choice çš„ä¸»åŠ¨å‡½æ•°æ‰§è¡Œï¼ˆæ”¯æŒ per-request MCP ç«¯ç‚¹ï¼‰
    try:
        # å°† mcp_endpoint æ³¨å…¥ utils.maybe_execute_tools çš„æ³¨å†Œé€»è¾‘
        if hasattr(request, 'mcp_endpoint') and request.mcp_endpoint:
            from .tools_registry import register_runtime_tools
            register_runtime_tools(getattr(request, 'tools', None), request.mcp_endpoint)
        tool_exec_results = await maybe_execute_tools(request.messages, request.tools, getattr(request, 'tool_choice', None))
    except Exception:
        tool_exec_results = None
    check_client_disconnected("After Prompt Prep")
    # å°†ç»“æœå†…è”åˆ°æç¤ºæœ«å°¾ï¼Œä¾›ç½‘é¡µç«¯ä¸€å¹¶æäº¤
    if tool_exec_results:
        try:
            for res in tool_exec_results:
                name = res.get('name')
                args = res.get('arguments')
                result_str = res.get('result')
                prepared_prompt += f"\n---\nå·¥å…·æ‰§è¡Œ: {name}\nå‚æ•°:\n{args}\nç»“æœ:\n{result_str}\n"
        except Exception:
            pass
    # è‹¥é…ç½®ä»…æ”¶é›†å½“å‰ç”¨æˆ·æ¶ˆæ¯é™„ä»¶ï¼Œåˆ™åœ¨æ­¤è¿‡æ»¤é™„ä»¶
    try:
        if ONLY_COLLECT_CURRENT_USER_ATTACHMENTS:
            latest_user = None
            for msg in reversed(request.messages or []):
                if getattr(msg, 'role', None) == 'user':
                    latest_user = msg
                    break
            if latest_user is not None:
                filtered: List[str] = []
                from api_utils.utils import extract_data_url_to_local
                from urllib.parse import urlparse, unquote
                import os
                # æ”¶é›†è¯¥æ¡ user æ¶ˆæ¯ä¸Šçš„ data:/file:/ç»å¯¹è·¯å¾„ï¼ˆå­˜åœ¨çš„ï¼‰
                content = getattr(latest_user, 'content', None)
                # ç»Ÿä¸€ä» messages é™„ä»¶å­—æ®µæŠ½å–
                for key in ('attachments', 'images', 'files', 'media'):
                    arr = getattr(latest_user, key, None)
                    if not isinstance(arr, list):
                        continue
                    for it in arr:
                        url_value = None
                        if isinstance(it, str):
                            url_value = it
                        elif isinstance(it, dict):
                            url_value = it.get('url') or it.get('path')
                        url_value = (url_value or '').strip()
                        if not url_value:
                            continue
                        if url_value.startswith('data:'):
                            fp = extract_data_url_to_local(url_value)
                            if fp:
                                filtered.append(fp)
                        elif url_value.startswith('file:'):
                            parsed = urlparse(url_value)
                            lp = unquote(parsed.path)
                            if os.path.exists(lp):
                                filtered.append(lp)
                        elif os.path.isabs(url_value) and os.path.exists(url_value):
                            filtered.append(url_value)
                images_list = filtered
    except Exception:
        pass

    return prepared_prompt, images_list

async def _handle_response_processing(
    req_id: str,
    request: ChatCompletionRequest,
    page: AsyncPage,
    context: RequestContext,
    result_future: Future,
    submit_button_locator: Locator,
    check_client_disconnected: Callable,
) -> Optional[Tuple[Event, Locator, Callable]]:
    """å¤„ç†å“åº”ç”Ÿæˆ"""
    from server import logger
    
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨è¾…åŠ©æµ
    from config import get_environment_variable
    stream_port = get_environment_variable('STREAM_PORT')
    use_stream = stream_port != '0'
    
    if use_stream:
        return await _handle_auxiliary_stream_response(req_id, request, context, result_future, submit_button_locator, check_client_disconnected)
    else:
        return await _handle_playwright_response(req_id, request, page, context, result_future, submit_button_locator, check_client_disconnected)


async def _handle_auxiliary_stream_response(
    req_id: str,
    request: ChatCompletionRequest,
    context: RequestContext,
    result_future: Future,
    submit_button_locator: Locator,
    check_client_disconnected: Callable,
) -> Optional[Tuple[Event, Locator, Callable]]:
    """è¾…åŠ©æµå“åº”å¤„ç†è·¯å¾„ï¼šè´Ÿè´£å°† STREAM_QUEUE çš„æ•°æ®è½¬æ¢ä¸º OpenAI å…¼å®¹ SSE/JSONã€‚

    - æµå¼æ¨¡å¼ï¼šè¿”å› StreamingResponseï¼Œé€æ­¥æ¨é€ delta ä¸æœ€ç»ˆ usageã€‚
    - éæµå¼æ¨¡å¼ï¼šèšåˆæœ€ç»ˆå†…å®¹ä¸å‡½æ•°è°ƒç”¨ï¼Œè¿”å› JSONResponseã€‚
    """
    from server import logger
    
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')
    
    # å…¼å®¹æ—§é€»è¾‘çš„éšæœºIDå‡½æ•°ç§»é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨ _random_id()

    if is_streaming:
        try:
            completion_event = Event()
            # ä½¿ç”¨ç”Ÿæˆå™¨ä½œä¸ºå“åº”ä½“ï¼Œäº¤ç”± FastAPI è¿›è¡Œ SSE æ¨é€
            stream_gen_func = gen_sse_from_aux_stream(
                req_id,
                request,
                current_ai_studio_model_id or MODEL_NAME,
                check_client_disconnected,
                completion_event,
            )
            if not result_future.done():
                result_future.set_result(StreamingResponse(stream_gen_func, media_type="text/event-stream"))
            else:
                if not completion_event.is_set():
                    completion_event.set()
            
            return completion_event, submit_button_locator, check_client_disconnected

        except Exception as e:
            logger.error(f"[{req_id}] ä»é˜Ÿåˆ—è·å–æµå¼æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            if completion_event and not completion_event.is_set():
                completion_event.set()
            raise

    else:  # éæµå¼
        content = None
        reasoning_content = None
        functions = None
        final_data_from_aux_stream = None

        # éæµå¼ï¼šæ¶ˆè´¹è¾…åŠ©é˜Ÿåˆ—çš„æœ€ç»ˆç»“æœå¹¶ç»„è£… JSON å“åº”
        async for raw_data in use_stream_response(req_id):
            check_client_disconnected(f"éæµå¼è¾…åŠ©æµ - å¾ªç¯ä¸­ ({req_id}): ")
            
            # ç¡®ä¿ data æ˜¯å­—å…¸ç±»å‹
            if isinstance(raw_data, str):
                try:
                    data = json.loads(raw_data)
                except json.JSONDecodeError:
                    logger.warning(f"[{req_id}] æ— æ³•è§£æéæµå¼æ•°æ®JSON: {raw_data}")
                    continue
            elif isinstance(raw_data, dict):
                data = raw_data
            else:
                logger.warning(f"[{req_id}] éæµå¼æœªçŸ¥æ•°æ®ç±»å‹: {type(raw_data)}")
                continue
            
            # ç¡®ä¿æ•°æ®æ˜¯å­—å…¸ç±»å‹
            if not isinstance(data, dict):
                logger.warning(f"[{req_id}] éæµå¼æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹: {data}")
                continue
                
            final_data_from_aux_stream = data
            if data.get("done"):
                content = data.get("body")
                reasoning_content = data.get("reason")
                functions = data.get("function")
                break
        
        if final_data_from_aux_stream and final_data_from_aux_stream.get("reason") == "internal_timeout":
            logger.error(f"[{req_id}] éæµå¼è¯·æ±‚é€šè¿‡è¾…åŠ©æµå¤±è´¥: å†…éƒ¨è¶…æ—¶")
            raise HTTPException(status_code=502, detail=f"[{req_id}] è¾…åŠ©æµå¤„ç†é”™è¯¯ (å†…éƒ¨è¶…æ—¶)")

        if final_data_from_aux_stream and final_data_from_aux_stream.get("done") is True and content is None:
             logger.error(f"[{req_id}] éæµå¼è¯·æ±‚é€šè¿‡è¾…åŠ©æµå®Œæˆä½†æœªæä¾›å†…å®¹")
             raise HTTPException(status_code=502, detail=f"[{req_id}] è¾…åŠ©æµå®Œæˆä½†æœªæä¾›å†…å®¹")

        model_name_for_json = current_ai_studio_model_id or MODEL_NAME
        message_payload = {"role": "assistant", "content": content}
        finish_reason_val = "stop"

        if functions and len(functions) > 0:
            tool_calls_list = []
            for func_idx, function_call_data in enumerate(functions):
                tool_calls_list.append({
                    "id": f"call_{_random_id()}",
                    "index": func_idx,
                    "type": "function",
                    "function": {
                        "name": function_call_data["name"],
                        "arguments": json.dumps(function_call_data["params"]),
                    },
                })
            message_payload["tool_calls"] = tool_calls_list
            finish_reason_val = "tool_calls"
            message_payload["content"] = None

        if reasoning_content:
            message_payload["reasoning_content"] = reasoning_content

        usage_stats = calculate_usage_stats(
            [msg.model_dump() for msg in request.messages],
            content or "",
            reasoning_content,
        )

        response_payload = build_chat_completion_response_json(
            req_id,
            model_name_for_json,
            message_payload,
            finish_reason_val,
            usage_stats,
            system_fingerprint="camoufox-proxy",
            seed=request.seed if hasattr(request, 'seed') else None,
            response_format=(request.response_format if hasattr(request, 'response_format') else None),
        )

        if not result_future.done():
            result_future.set_result(JSONResponse(content=response_payload))
        return None


async def _handle_playwright_response(req_id: str, request: ChatCompletionRequest, page: AsyncPage, 
                                    context: dict, result_future: Future, submit_button_locator: Locator, 
                                    check_client_disconnected: Callable) -> Optional[Tuple[Event, Locator, Callable]]:
    """ä½¿ç”¨Playwrightå¤„ç†å“åº”"""
    from server import logger
    
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')
    
    await locate_response_elements(page, req_id, logger, check_client_disconnected)

    check_client_disconnected("After Response Element Located: ")

    if is_streaming:
        completion_event = Event()
        stream_gen_func = gen_sse_from_playwright(
            page,
            logger,
            req_id,
            current_ai_studio_model_id or MODEL_NAME,
            request,
            check_client_disconnected,
            completion_event,
        )
        if not result_future.done():
            result_future.set_result(StreamingResponse(stream_gen_func, media_type="text/event-stream"))
        
        return completion_event, submit_button_locator, check_client_disconnected
    else:
        # ä½¿ç”¨PageControllerè·å–å“åº”
        page_controller = PageController(page, logger, req_id)
        final_content = await page_controller.get_response(check_client_disconnected) or ""
        final_content, parsed_tool_calls = extract_tool_calls_from_text(final_content)
        
        # è®¡ç®—tokenä½¿ç”¨ç»Ÿè®¡
        usage_stats = calculate_usage_stats(
            [msg.model_dump() for msg in request.messages],
            final_content,
            ""  # Playwrightæ¨¡å¼æ²¡æœ‰reasoning content
        )
        logger.info(f"[{req_id}] Playwrightéæµå¼è®¡ç®—çš„tokenä½¿ç”¨ç»Ÿè®¡: {usage_stats}")

        # ç»Ÿä¸€ä½¿ç”¨æ„é€ å™¨ç”Ÿæˆ OpenAI å…¼å®¹å“åº”
        model_name_for_json = current_ai_studio_model_id or MODEL_NAME
        message_payload = {"role": "assistant", "content": final_content}
        finish_reason_val = "stop"
        if parsed_tool_calls:
            formatted_calls = format_tool_calls_for_response(parsed_tool_calls)
            if formatted_calls:
                message_payload["tool_calls"] = formatted_calls
                finish_reason_val = "tool_calls"

        response_payload = build_chat_completion_response_json(
            req_id,
            model_name_for_json,
            message_payload,
            finish_reason_val,
            usage_stats,
            system_fingerprint="camoufox-proxy",
            seed=request.seed if hasattr(request, 'seed') else None,
            response_format=(request.response_format if hasattr(request, 'response_format') else None),
        )
        
        if not result_future.done():
            result_future.set_result(JSONResponse(content=response_payload))
        
        return None


async def _cleanup_request_resources(req_id: str, disconnect_check_task: Optional[asyncio.Task], 
                                   completion_event: Optional[Event], result_future: Future, 
                                   is_streaming: bool) -> None:
    """æ¸…ç†è¯·æ±‚èµ„æº"""
    from server import logger
    from config import UPLOAD_FILES_DIR
    import os, shutil
    
    if disconnect_check_task and not disconnect_check_task.done():
        disconnect_check_task.cancel()
        try: 
            await disconnect_check_task
        except asyncio.CancelledError: 
            pass
        except Exception as task_clean_err: 
            logger.error(f"[{req_id}] æ¸…ç†ä»»åŠ¡æ—¶å‡ºé”™: {task_clean_err}")
    
    logger.info(f"[{req_id}] å¤„ç†å®Œæˆã€‚")

    # æ¸…ç†æœ¬æ¬¡è¯·æ±‚çš„ä¸Šä¼ å­ç›®å½•ï¼Œé¿å…ç£ç›˜ç´¯ç§¯
    try:
        req_dir = os.path.join(UPLOAD_FILES_DIR, req_id)
        if os.path.isdir(req_dir):
            shutil.rmtree(req_dir, ignore_errors=True)
            logger.info(f"[{req_id}] å·²æ¸…ç†è¯·æ±‚ä¸Šä¼ ç›®å½•: {req_dir}")
    except Exception as clean_err:
        logger.warning(f"[{req_id}] æ¸…ç†ä¸Šä¼ ç›®å½•å¤±è´¥: {clean_err}")
    
    if is_streaming and completion_event and not completion_event.is_set() and (result_future.done() and result_future.exception() is not None):
         logger.warning(f"[{req_id}] æµå¼è¯·æ±‚å¼‚å¸¸ï¼Œç¡®ä¿å®Œæˆäº‹ä»¶å·²è®¾ç½®ã€‚")
         completion_event.set()


async def _process_request_refactored(
    req_id: str,
    request: ChatCompletionRequest,
    http_request: Request,
    result_future: Future
) -> Optional[Tuple[Event, Locator, Callable[[str], bool]]]:
    """æ ¸å¿ƒè¯·æ±‚å¤„ç†å‡½æ•° - é‡æ„ç‰ˆæœ¬"""

    # ä¼˜åŒ–ï¼šåœ¨å¼€å§‹ä»»ä½•å¤„ç†å‰ä¸»åŠ¨æ£€æµ‹å®¢æˆ·ç«¯è¿æ¥çŠ¶æ€
    from server import logger
    from config import get_environment_variable

    is_connected = await _test_client_connection(req_id, http_request)
    if not is_connected:
        logger.info(f"[{req_id}] âœ… æ ¸å¿ƒå¤„ç†å‰æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼Œæå‰é€€å‡ºèŠ‚çœèµ„æº")
        if not result_future.done():
            result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] å®¢æˆ·ç«¯åœ¨å¤„ç†å¼€å§‹å‰å·²æ–­å¼€è¿æ¥"))
        return None

    stream_port = get_environment_variable('STREAM_PORT')
    use_stream = stream_port != '0'
    if use_stream:
        logger.info(f"[{req_id}] ğŸ”§ è¯·æ±‚å¼€å§‹å‰æ¸…ç©ºæµå¼é˜Ÿåˆ—ï¼ˆé˜²æ­¢æ®‹ç•™æ•°æ®ï¼‰...")
        try:
            from api_utils import clear_stream_queue
            await clear_stream_queue()
            logger.info(f"[{req_id}] âœ… æµå¼é˜Ÿåˆ—å·²æ¸…ç©º")
        except Exception as clear_err:
            logger.warning(f"[{req_id}] âš ï¸ æ¸…ç©ºæµå¼é˜Ÿåˆ—æ—¶å‡ºé”™: {clear_err}")

    context = await _initialize_request_context(req_id, request)
    context = await _analyze_model_requirements(req_id, context, request)
    
    client_disconnected_event, disconnect_check_task, check_client_disconnected = await _setup_disconnect_monitoring(
        req_id, http_request, result_future
    )
    
    page = context['page']
    submit_button_locator = page.locator(SUBMIT_BUTTON_SELECTOR) if page else None
    completion_event = None
    
    try:
        await _validate_page_status(req_id, context, check_client_disconnected)
        
        page_controller = PageController(page, context['logger'], req_id)

        await _handle_model_switching(req_id, context, check_client_disconnected)
        await _handle_parameter_cache(req_id, context)
        
        prepared_prompt,image_list = await _prepare_and_validate_request(req_id, request, check_client_disconnected)
        # é¢å¤–åˆå¹¶é¡¶å±‚ä¸æ¶ˆæ¯çº§ attachments/filesï¼ˆå…¼å®¹å†å²è®°å½•ï¼‰å·²åœ¨ä¸‹æ–¹å¤„ç†ï¼›æ­¤å¤„ç¡®ä¿è·¯å¾„å­˜åœ¨
        try:
            import os
            valid_images = []
            for p in image_list:
                if isinstance(p, str) and p and os.path.isabs(p) and os.path.exists(p):
                    valid_images.append(p)
            if len(valid_images) != len(image_list):
                from server import logger
                logger.warning(f"[{req_id}] è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„é™„ä»¶è·¯å¾„: {set(image_list) - set(valid_images)}")
            image_list = valid_images
        except Exception:
            pass
        # å…¼å®¹: é¡¶å±‚ä¸æ¶ˆæ¯çº§é™„ä»¶å­—æ®µåˆå¹¶åˆ°ä¸Šä¼ åˆ—è¡¨ï¼ˆä»… data:/file:/ç»å¯¹è·¯å¾„ï¼‰
        # é™„ä»¶æ¥æºç­–ç•¥ï¼šä»…æ¥å—å½“å‰è¯·æ±‚æ˜¾å¼æä¾›çš„ data:/file:/ç»å¯¹è·¯å¾„ï¼ˆå­˜åœ¨çš„ï¼‰
        try:
            from api_utils.utils import extract_data_url_to_local
            from urllib.parse import urlparse, unquote
            import os
            # é¡¶å±‚ attachments
            top_level_atts = getattr(request, 'attachments', None)
            if isinstance(top_level_atts, list) and len(top_level_atts) > 0:
                for it in top_level_atts:
                    url_value = None
                    if isinstance(it, str):
                        url_value = it
                    elif isinstance(it, dict):
                        url_value = it.get('url') or it.get('path')
                    url_value = (url_value or '').strip()
                    if not url_value:
                        continue
                    if url_value.startswith('data:'):
                        fp = extract_data_url_to_local(url_value, req_id=req_id)
                        if fp:
                            image_list.append(fp)
                    elif url_value.startswith('file:'):
                        parsed = urlparse(url_value)
                        lp = unquote(parsed.path)
                        if os.path.exists(lp):
                            image_list.append(lp)
                    elif os.path.isabs(url_value) and os.path.exists(url_value):
                        image_list.append(url_value)
            # æ¶ˆæ¯çº§ attachments/images/files/mediaï¼ˆå…¨é‡æ”¶é›†ï¼Œä½†ä»…ä¿ç•™æœ‰æ•ˆæœ¬åœ°/dataï¼‰
            for msg in (request.messages or []):
                for key in ('attachments', 'images', 'files', 'media'):
                    arr = getattr(msg, key, None)
                    if not isinstance(arr, list):
                        continue
                    for it in arr:
                        url_value = None
                        if isinstance(it, str):
                            url_value = it
                        elif isinstance(it, dict):
                            url_value = it.get('url') or it.get('path')
                        url_value = (url_value or '').strip()
                        if not url_value:
                            continue
                        if url_value.startswith('data:'):
                            fp = extract_data_url_to_local(url_value, req_id=req_id)
                            if fp:
                                image_list.append(fp)
                        elif url_value.startswith('file:'):
                            parsed = urlparse(url_value)
                            lp = unquote(parsed.path)
                            if os.path.exists(lp):
                                image_list.append(lp)
                        elif os.path.isabs(url_value) and os.path.exists(url_value):
                            image_list.append(url_value)
        except Exception:
            pass

        # ä½¿ç”¨PageControllerå¤„ç†é¡µé¢äº¤äº’
        # æ³¨æ„ï¼šèŠå¤©å†å²æ¸…ç©ºå·²ç§»è‡³é˜Ÿåˆ—å¤„ç†é”é‡Šæ”¾åæ‰§è¡Œ

        await page_controller.adjust_parameters(
            request.model_dump(exclude_none=True), # ä½¿ç”¨ exclude_none=True é¿å…ä¼ é€’Noneå€¼
            context['page_params_cache'],
            context['params_cache_lock'],
            context['model_id_to_use'],
            context['parsed_model_list'],
            check_client_disconnected
        )

        # ä¼˜åŒ–ï¼šåœ¨æäº¤æç¤ºå‰å†æ¬¡æ£€æŸ¥å®¢æˆ·ç«¯è¿æ¥ï¼Œé¿å…ä¸å¿…è¦çš„åå°è¯·æ±‚
        check_client_disconnected("æäº¤æç¤ºå‰æœ€ç»ˆæ£€æŸ¥")

        await page_controller.submit_prompt(prepared_prompt,image_list, check_client_disconnected)
        
        # å“åº”å¤„ç†ä»ç„¶éœ€è¦åœ¨è¿™é‡Œï¼Œå› ä¸ºå®ƒå†³å®šäº†æ˜¯æµå¼è¿˜æ˜¯éæµå¼ï¼Œå¹¶è®¾ç½®future
        response_result = await _handle_response_processing(
            req_id, request, page, context, result_future, submit_button_locator, check_client_disconnected
        )
        
        if response_result:
            completion_event, _, _ = response_result
        
        return completion_event, submit_button_locator, check_client_disconnected
        
    except ClientDisconnectedError as disco_err:
        context['logger'].info(f"[{req_id}] æ•è·åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ä¿¡å·: {disco_err}")
        if not result_future.done():
             result_future.set_exception(client_disconnected(req_id, "Client disconnected during processing."))
    except HTTPException as http_err:
        context['logger'].warning(f"[{req_id}] æ•è·åˆ° HTTP å¼‚å¸¸: {http_err.status_code} - {http_err.detail}")
        if not result_future.done():
            result_future.set_exception(http_err)
    except PlaywrightAsyncError as pw_err:
        context['logger'].error(f"[{req_id}] æ•è·åˆ° Playwright é”™è¯¯: {pw_err}")
        await save_error_snapshot(f"process_playwright_error_{req_id}")
        if not result_future.done():
            result_future.set_exception(upstream_error(req_id, f"Playwright interaction failed: {pw_err}"))
    except Exception as e:
        context['logger'].exception(f"[{req_id}] æ•è·åˆ°æ„å¤–é”™è¯¯")
        await save_error_snapshot(f"process_unexpected_error_{req_id}")
        if not result_future.done():
            result_future.set_exception(server_error(req_id, f"Unexpected server error: {e}"))
    finally:
        await _cleanup_request_resources(req_id, disconnect_check_task, completion_event, result_future, request.stream)
